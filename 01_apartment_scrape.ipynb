{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "208681d7-f6b0-45b4-b3cf-2f8ba79650e6",
   "metadata": {},
   "source": [
    "# Apartments.com listing scraper\n",
    "\n",
    "This notebook will read the listings map page of apartments.com and pull out all of the rental properties details to be used in the modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9400daa9-e1ab-42a2-ad11-25ac03362649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b07ae42-ad1b-42e3-b242-02996456f5d1",
   "metadata": {},
   "source": [
    "Apartments.com employs some amount of anti-scraping technology. To circumvent the anti-scraping measures, for educational purposes only of course, we provide bogus User Agents in the http request as well as routing the request through a list of proxies.\n",
    "\n",
    "The proxies were obtained from https://www.us-proxy.org/ . The list included in this repository is only the first three entries on the list. There is a chance that any given proxy server may not be usable, so it is important to have a large list of potential proxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12bdef6f-2608-49d5-9c46-d3b74f81076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "proxies = []\n",
    "with open(\"proxies.txt\", \"r\") as fin:\n",
    "    for line in fin.readlines():\n",
    "        proxies.append(line.strip(\"\\n\"))\n",
    "\n",
    "user_agents = list(string.printable[:62])\n",
    "headers  = {'User-Agent':np.random.choice(user_agents)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4f34ea-c278-4606-9c57-36b199602290",
   "metadata": {},
   "source": [
    "These helper functions will later be used to process the scraped HTML code and pull out relevant listing details and add them to a listing dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ec54095-c986-4824-aa60-e7726986d3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(my_string):\n",
    "    brackets = ['()', '{}', '[]']\n",
    "    while any(x in my_string for x in brackets):\n",
    "        for br in brackets:\n",
    "            my_string = my_string.replace(br, '')\n",
    "    return not my_string\n",
    "\n",
    "def get_propid(soup, listing_props):\n",
    "    try:\n",
    "        attrs = soup.find(\"main\").attrs\n",
    "        listing_props[\"listingid\"] = attrs[\"data-listingid\"]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def get_basic(soup, listing_props):\n",
    "    basic_table = soup.find(id=\"priceBedBathAreaInfoWrapper\")\n",
    "    for prop in [\"Monthly Rent\", \"Bedrooms\", \"Bathrooms\", \"Square Feet\"]:\n",
    "        try:\n",
    "            listing_props[prop] = basic_table.find(text=prop).findNext().contents[0]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "def get_zip(soup, listing_props):        \n",
    "    try:\n",
    "        zipcode = soup.find(class_=\"stateZipContainer\").findNext().findNext().contents[0]\n",
    "        listing_props[\"ZIP\"] = int(zipcode)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "def get_deposit(soup, listing_props):\n",
    "    try:\n",
    "        deposit = soup.find(class_=\"detailsTextWrapper leaseDepositLabel\").findNext().findNext().contents[0]\n",
    "        listing_props[\"Deposit\"] = deposit\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def get_scores(soup, listing_props):\n",
    "    for score in [\"walkScore\", \"transitScore\"]:\n",
    "        try:\n",
    "            attrs = soup.find(class_=f\"component-header ratingCol {score}\").attrs\n",
    "            listing_props[score] = attrs[\"data-score\"]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "def get_coords(soup, listing_props):\n",
    "    try:\n",
    "        listing_props[\"latitude\"] = soup.find_all(property=\"place:location:latitude\")[0][\"content\"]\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        listing_props[\"longitude\"] = soup.find_all(property=\"place:location:longitude\")[0][\"content\"]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def get_neighborhood(soup, listing_props):\n",
    "    try:\n",
    "        listing_props[\"neighborhood\"] = soup.find_all(class_=\"neighborhood\")[0].contents[0]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#Note: this was just a first attempt at scraping the pet policy. In our work, we found that the actualy pet listings would require processing the \n",
    "#body text of the listing as well as a more robust treatment of the built-in pet policy table.\n",
    "def get_pet(soup, listing_props):\n",
    "    try:\n",
    "        pet = soup.find(class_=\"feePolicyTitle petPolicyTitle\").contents[0].lower()\n",
    "        if \"no pet\" in pet:\n",
    "            listing_props[\"Pet\"] = 0\n",
    "        if \"dog\" in pet:\n",
    "            listing_props[\"Pet\"] = 2\n",
    "        if \"cat\" in pet:\n",
    "            listing_props[\"Pet\"] = 3\n",
    "        if (\"dog\" in pet) and (\"cat\" in pet):\n",
    "            listing_props[\"Pet\"] = 5\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdad5b9d-e228-46a1-bbe0-ada0c7ff983b",
   "metadata": {},
   "source": [
    "Here we scrape the listing map and pull out all of the URLs for the apartment listings. In the loop, we pull only 28 pages of listings. The number 28 \n",
    "corresponds to the number of listings pages which were available when we began the project. This number will change depending on the location,\n",
    "apartment selection criteria, and naturally with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cda1f5f-ba6a-4620-b2d6-9363af72d57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_url = 'https://www.apartments.com/indianapolis-in/over-200/'\n",
    "rent_urls = []\n",
    "\n",
    "listing_links = []\n",
    "\n",
    "for i in range(1,29):\n",
    "    url = base_url+str(i)\n",
    "    \n",
    "    headers  = {'User-Agent':np.random.choice(user_agents)} #Change the user agent to avoid anti-crawler measures\n",
    "\n",
    "    res  = requests.get(url, timeout = 10, headers = headers)\n",
    "    soup = BeautifulSoup(res.content, 'lxml')\n",
    "\n",
    "    links = soup.find_all(\"a\")\n",
    "    for link in links:\n",
    "        link = link.get(\"href\")\n",
    "\n",
    "        try:\n",
    "            #Regex match to the form of the apartment listing id.\n",
    "            if re.search(\"\\/[a-zA-Z0-9]{7}\\/$\", link) and (\"sitemap\" not in link):\n",
    "                listing_links.append(link)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    time.sleep(np.random.uniform(1, 10)) #Wait for a short moment before submitting the next request.\n",
    "\n",
    "#Extract unique listings and save to a text time to skip this step in subsequent runs.\n",
    "unique_listings = list(set(listing_links))\n",
    "\n",
    "with open(\"listings.txt\", \"w\") as out:\n",
    "    for link in unique_listings:\n",
    "        out.write(link+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e30f2d7e-d98a-4b55-b235-d080c5a79229",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6912/4195569705.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#If listing links already scraped, just run this cell to get the listing links loaded.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"listings.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"listings.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0munique_listings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "#If listing links already scraped, just run this cell to get the listing links loaded.\n",
    "if os.path.exists(\"listings.txt\"):\n",
    "    with open(\"listings.txt\", \"r\") as fin:\n",
    "        unique_listings = [line.replace(\"\\n\", \"\") for line in fin.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d222a9-ee64-4952-afeb-8aadc4af1d81",
   "metadata": {},
   "source": [
    "Here we scrape each of the listing links to extract the rental listing properties. This cell can take a while as there are hundreds of listings\n",
    "and we include a generous pause between each HTTP request (~20 seconds) to prevent being flagged as a bot.\n",
    "\n",
    "The workflow for this is attempt to grab the page, trying a few times if the request times out or breaks, create a dictionary to hold the listings properties,\n",
    "call the helper functions to extract the properties, and add to the dataset. We then convert the scraped data into a dataframe and save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0317571c-6071-4be0-9279-c732c8536d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "listings = []\n",
    "#We limit this cell to the first 50 listings for testing purposes.\n",
    "for listing in unique_listings[:50]:\n",
    "    headers  = {'User-Agent':np.random.choice(user_agents)} #Anti-anti-crawler\n",
    "    \n",
    "    success = False\n",
    "    attempts = 1\n",
    "    while not success:\n",
    "        try:\n",
    "            proxy = np.random.choice(proxies)\n",
    "            print(f\"Reading: {listing} \\n on proxy server: {proxy}\")\n",
    "            res  = requests.get(listing, timeout = 10, headers = headers,\n",
    "                                proxies = {\"proxies\": proxy})\n",
    "            soup = BeautifulSoup(res.content, 'lxml')\n",
    "            success = True\n",
    "        except TimeoutError:\n",
    "            #We can time out for a few reasons: bad proxy server or we got flagged. Wait a bit and try again with different headers and proxy.\n",
    "            print(f\"Timed out on attempt {attempts}, trying again...\")\n",
    "            time.sleep(5)\n",
    "            attempts += 1\n",
    "            if attempts==5: break\n",
    "            \n",
    "\n",
    "    listing_props = {\"url\": listing}\n",
    "    \n",
    "    get_propid(soup, listing_props)\n",
    "    get_zip(soup, listing_props)\n",
    "    get_basic(soup, listing_props)\n",
    "    get_scores(soup, listing_props)\n",
    "    get_deposit(soup, listing_props)\n",
    "    get_coords(soup, listing_props)\n",
    "    get_pet(soup, listing_props)\n",
    "    get_neighborhood(soup, listing_props)\n",
    "    \n",
    "    listings.append(listing_props.copy())\n",
    "    time.sleep(np.random.uniform(5,30))\n",
    "\n",
    "#Finally, save the data so we don't have to scrape again. Whole scraping process took ~4 hours!\n",
    "df = pd.DataFrame(listings)\n",
    "df.to_excel(\"apartment_dotcom.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
